{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nTOCO-3tlQcP",
        "eq5PO5mIL7Nw",
        "7YEzlrA0MCJu",
        "k7jRT4B6MMWM",
        "oCqFJWNqNTGk",
        "ZXYjnVnbExUl",
        "yhzvrLFeN1EM",
        "63JoEwpnPeHt",
        "5osjHFHlsEBM",
        "nlLhApHAPryr"
      ],
      "authorship_tag": "ABX9TyNZZj+VS+VPPxCjboFiYXMi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Comparing corpora using distributional semantics methods**"
      ],
      "metadata": {
        "id": "JD0bJ8r2je2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Academic Year: 2022/23\n",
        "\n",
        "\n",
        "Cognitive Science Program, University of Warsaw, Poland\n",
        "\n",
        "**Abstract**:\n",
        "\n",
        "Distributional semantics is a powerful concept that catches the meaning of linguistic units (here words) based on their occurrence patterns in corpora. The development of this field has resulted in a number of models that use various techniques to model meaning of texts by mapping them onto a multidimensional space. High-quality vectors (embeddings) should map various relations between linguistic units, such as the relation of similarity. So, similar words should be close to each other in space. Thanks to this property, it is possible to cluster similar words in order, for example, to find topics occurring in a given corpus.\n",
        "\n",
        "The research question in my work was to find a method for extracting topics from Polish language corpora that uses embeddings clustering and that would be best suited for use in an application that compares corpora semantically. Thus, the evaluation of methods was done in terms of accuracy, but also in terms of ease and speed.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WVJQK5u8aAXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description of the work**\n",
        "\n",
        "This work focuses on conducting a comprehensive comparison of three distinct models for topic modeling tasks, employing diverse corpora, clustering algorithms and pretrained language models.\n",
        "\n",
        "It specifically focuses on conducting a comparison of three models for topic modeling tasks using Polish corpora. The primary objective is to evaluate the performance of these algorithms on Polish data, aiming to identify the most suitable approach that best captures the internal structure of the text.\n",
        "\n",
        "Throughout this study, a key consideration was given to algorithm and application simplicity. The goal was to explore models that are not only effective but also easy to implement and interpret, allowing for practical and efficient utilization.\n",
        "\n",
        "The selected corpora encompass:\n",
        "\n",
        "1. A corpus consisting of journalistic texts.\n",
        "2. A corpus containing literary and poetic texts.\n",
        "3. A corpus comprised of non-fiction, guidebooks, and didactic texts.\n",
        "4. A corpus comprising parliamentary texts.\n",
        "5. Volume I of the Polish novel \"Lalka.\"\n",
        "6. Volume II of the Polish novel \"Lalka.\"\n",
        "\n",
        "These chosen corpora encompass both sets that are intentionally designed to be closely related to each other (e.g., corpora 5, 6, and 2) and those that exhibit semantic distance (e.g., corpus 1 and corpus 6).\n",
        "\n",
        "The employed models are as follows:\n",
        "\n",
        "1. A pretrained Fasttext model, applied on lemmatized corpora, and using KMeans as the clustering algorithm.\n",
        "2. The BERTopic model, trained on sentences extracted from corpora, utilizing vectors from the SentenceTransformers library, and employing HDBSCAN for clustering.\n",
        "3. Analogiacal BERTopic model, but using a pretrained Spacy model for Polish language.\n",
        "4. The Top2Vec model, trained from scratch on sentences derived corpora.\n",
        "5. Analogical Top2Vec model, but using SentenceTransformers.\n",
        "\n",
        "Throughout the experimentation, different combinations of hyperparameters were evaluated for each of these models. Subsequently, the identified topics were subjected to an automated evaluation process, wherein metrics such as coherence, topic diversity, and cosine similarity were calculated.\n",
        "\n",
        "It is important to note that this notebook is organized in a manner that facilitates a step-by-step approach rather than a one-time exhaustive process, resulting in some code repetition to enhance readability and comprehension. The order of the code mirrors the sequence of experiments undertaken.\n",
        "\n",
        "This notebook contains the code that was used to generate the results that were ultimately included in my thesis."
      ],
      "metadata": {
        "id": "uSArLjtv8_fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "KYaUaadPp1fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python version 3.10.12\n",
        "\n",
        "bs4==4.11.2\n",
        "\n",
        "spacy==3.5.4\n",
        "\n",
        "umap-learn==0.5.3\n",
        "\n",
        "fasttext==v0.9.1\n",
        "\n",
        "sklearn==1.2.2\n",
        "\n",
        "seaborn==0.12.2\n",
        "\n",
        "pandas==1.5.3\n",
        "\n",
        "matplotlib==3.7\n",
        "\n",
        "tqdm==4.65.0\n",
        "\n",
        "numpy==1.22.4\n",
        "\n",
        "sentence_transformers==2.2.2\n",
        "\n",
        "hdbscan==0.8.33\n",
        "\n",
        "bertopic==v0.14.0\n",
        "\n",
        "keybert==v0.7.0\n",
        "\n",
        "jax>=0.4.9\n",
        "\n",
        "bertopic[spacy]==v0.14.0\n",
        "\n",
        "keybert[spacy]==v0.7.0\n",
        "\n",
        "top2vec==1.0.29\n",
        "\n",
        "top2vec[sentence_transformers]==1.0.29\n",
        "\n",
        "gensim==4.3.1\n",
        "\n",
        "editdistance==0.6.2"
      ],
      "metadata": {
        "id": "E7py5Wqhp6Ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "vTPj5nhPw075"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFKQlyluVKuI"
      },
      "source": [
        "## Data fetching and dumping"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Here are two helper functions for loading and dumping corresponding files.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "i9tE696dBn9T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_xAXuxu18-8"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgw2Qm7zVMzC"
      },
      "outputs": [],
      "source": [
        "def load_file(file_name):\n",
        "  file_path = \".../topic-modelling-polish/\" + file_name + \".pkl\"\n",
        "  with open(file_path, \"rb\") as f:\n",
        "    fil = pickle.load(f)\n",
        "  return fil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vKgVTC3imrX"
      },
      "outputs": [],
      "source": [
        "def dump_result(result, result_name):\n",
        "  result_name = \".../topic-modelling-polish/\" + result_name + \".pkl\"\n",
        "\n",
        "  with open(result_name, \"wb\") as f:\n",
        "\n",
        "    pickle.dump(result,f )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9Ay9x7IVTaV"
      },
      "outputs": [],
      "source": [
        "#Saving the .txt version\n",
        "\n",
        "publ = load_file('corpora/pickle/original/publ')\n",
        "lit = load_file('corpora/pickle/original/lit')\n",
        "fakt_ind_nd = load_file('corpora/pickle/original/fakt_ind_nd')\n",
        "\n",
        "\n",
        "\n",
        "with open('.../topic-modelling-polish/corpora/txt/publ.txt','w',encoding='utf-8') as f:\n",
        "  f.writelines(publ)\n",
        "\n",
        "with open('.../topic-modelling-polish/corpora/txt/lit.txt','w',encoding='utf-8') as g:\n",
        "  g.writelines(lit)\n",
        "\n",
        "with open('.../topic-modelling-polish/corpora/txt/fakt_ind_nd.txt','w',encoding='utf-8') as h:\n",
        "  h.writelines(fakt_ind_nd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTOCO-3tlQcP"
      },
      "source": [
        "## Downloading 1-milion-word subcorpus of the National Corpus of Polish"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "[National Corpus of Polish](http://nkjp.pl/index.php?page=0&lang=1) is a balanced corpus of Polish language. In this work, I use a 1-milion-word sample of it that is easy available here: http://nkjp.pl/index.php?page=14&lang=1\n",
        "\n",
        "In this sample, there are texts coming from various registers according to the typology published in this book (only in Polish).\n",
        "\n",
        "https://www.researchgate.net/publication/262184393_Recznie_znakowany_milionowy_podkorpus_NKJP/link/00463536e8212b2a3f000000/download, p. 53\n",
        "\n",
        "Underneath, I present a slightly modified (and translated) table taken from the above book.\n",
        "\n",
        "Category | Rate   | Type\n",
        "-------- | ------ | ----\n",
        "Journals | 25.5%  | #typ_publ\n",
        "Other Periodicals | 23.5%  | #typ_publ\n",
        "Journalistic Books | 1.0%   | #typ_publ\n",
        "Fiction Literature | 16.0%  | #typ_lit, #typ_lit_poezja\n",
        "Non-Fiction Literature | 5.5%   | #typ_fakt\n",
        "Informative and Guide Type | 5.5%   | #typ_inf-por\n",
        "Educational and Didactic Type | 2.0%   | #typ_nd\n",
        "Online Interactive (Blogs, Forums, Usenet) | 3.5%   | #typ_net_interakt\n",
        "Online Non-Interactive (Static Pages, Wikipedia) | 3.5%   | #typ_net_nieinterakt\n",
        "Quasi-Spoken (Parliamentary Session Protocols) | 2.5%   | #typ_qmow\n",
        "Media Spoken | 2.5%   | #typ_media\n",
        "Conversational Spoken | 5.0%   | #typ_konwers\n",
        "Other Written Texts | 3.0%   | #typ_urzed\n",
        "Non-Fiction Unclassified Books | 1.0%   | #typ_nklas\n",
        " |   | #typ_listy\n",
        "\n",
        " ---\n"
      ],
      "metadata": {
        "id": "M6cS1pU1b1iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-milion-word subcorpus was the source for the three corpora used here in the comparisons:\n",
        "\n",
        "* **publ**\n",
        "  * corpus with around 426,000 tokens composed of journals, other periodicals, and journalistic books (typ_publ)  \n",
        "* **lit**\n",
        "  * corpus with around 204,000 tokens composed of piction literature (typ_lit, typ_poezja).\n",
        "* **fakt_ind_nd**\n",
        "  * corpus with around 160,000 tokens composed of non-ficton literature, informative and guide type, and educational and didactic type (typ_fakt, typ_inf_por, typ_nd)."
      ],
      "metadata": {
        "id": "Vv-6FV5-9Y1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir .../working/corpora/"
      ],
      "metadata": {
        "id": "XS4reuepypii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmB8p6fzLsat"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import pickle\n",
        "import tarfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IesLG-FXvx28"
      },
      "outputs": [],
      "source": [
        "file = tarfile.open('.../topic-modelling-polish/corpora/raw/NKJP-PodkorpusMilionowy-1.2.tar.gz')\n",
        "file.extractall('.../working/corpora/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m9NleGXmjr-"
      },
      "outputs": [],
      "source": [
        "#Here, I'm extracting a list of text registers from NKJP sample corpus\n",
        "lista_plikow = os.listdir('.../working/corpora/')\n",
        "\n",
        "\n",
        "lista_typow = []\n",
        "\n",
        "for l in lista_plikow:\n",
        "  if not re.search('[a-zA-Z]', l):\n",
        "\n",
        "    path = '.../working/corpora/' + l + '/header.xml'\n",
        "    with open(path, 'r') as f:\n",
        "      file = f.read()\n",
        "\n",
        "      soup = BeautifulSoup(file, 'xml')\n",
        "      types = soup.find('catRef')\n",
        "\n",
        "      typ = types.attrs['target']\n",
        "\n",
        "      if typ not in lista_typow:\n",
        "        lista_typow.append(typ)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIrZFfwm124o"
      },
      "outputs": [],
      "source": [
        "def extract_texts(path):\n",
        "  with open(path, 'r') as f:\n",
        "    file = f.read()\n",
        "\n",
        "    texts = []\n",
        "\n",
        "    soup = BeautifulSoup(file, 'xml')\n",
        "    text_objects = soup.find_all('ab')\n",
        "\n",
        "    for txt in text_objects:\n",
        "      texts.append(txt.get_text())\n",
        "\n",
        "    texts = \"\\n\".join(texts)\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBzPJ8Ov0gMP"
      },
      "outputs": [],
      "source": [
        "publ = []\n",
        "lit = []\n",
        "fakt_ind_nd = []\n",
        "\n",
        "for folder in lista_plikow:\n",
        "  if not re.search('[a-zA-Z]', folder):\n",
        "\n",
        "    path_header = '.../working/corpora/' + folder + '/header.xml'\n",
        "    path_text = '.../working/corpora/' + folder + '/text.xml'\n",
        "\n",
        "    with open(path_header, 'r') as g:\n",
        "      file_g = g.read()\n",
        "\n",
        "      soup_g = BeautifulSoup(file_g, 'xml')\n",
        "      types = soup_g.find('catRef')\n",
        "\n",
        "      typ = types.attrs['target']\n",
        "\n",
        "      if typ == \"#typ_publ\":\n",
        "        publ.append(extract_texts(path_text))\n",
        "      elif typ == \"#typ_lit\" or typ == \"#typ_lit_poezja\":\n",
        "        lit.append(extract_texts(path_text))\n",
        "      elif typ == \"#typ_fakt\" or typ == \"#typ_inf-por\" or typ == \"#typ_nd\":\n",
        "        fakt_ind_nd.append(extract_texts(path_text))\n",
        "\n",
        "with open(\".../topic-modelling-polish/corpora/pickle/original/publ.pkl\", \"wb\") as f:\n",
        "  pickle.dump(publ, f)\n",
        "\n",
        "with open(\".../topic-modelling-polish/corpora/pickle/original/lit.pkl\", \"wb\") as p:\n",
        "  pickle.dump(lit, p)\n",
        "\n",
        "with open(\".../topic-modelling-polish/corpora/pickle/original/fakt_ind_nd.pkl\", \"wb\") as t:\n",
        "  pickle.dump(fakt_ind_nd, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq5PO5mIL7Nw"
      },
      "source": [
        "## Downloading *Lalka* Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "*Lalka* is a 19th century Polish novel written by Boleslaw Prus. It consists of two volumes, each with 19 chapters. Volume I has around 154,000 tokens, while Volume II has around 173,000 tokens. I used it in comparisons as an example of texts that are very similar to each other.\n",
        "\n",
        "Throughout the remainder of this notebook, the reference term `lalka-tom-pierwszy` pertains to Volume One, and `lalka-tom-drugi` pertains to Volume Two.\n",
        "\n",
        "The novel *Lalka* was downloaded from the public domain book site [Wolne Lektury](https://wolnelektury.pl/katalog/lektura/lalka/).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JpC58eF3f0qO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsb4IY_aVuyG"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "from bs4 import BeautifulSoup\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCdfKfMjuftk"
      },
      "outputs": [],
      "source": [
        "with open('.../topic-modelling-polish/corpora/txt/lalka-tom-pierwszy.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "  raw_korpus = text.split('\\n\\n\\n\\n') #In this corpus, chapters are considered documents\n",
        "  raw_korpus = raw_korpus[2:23]\n",
        "  with open('.../topic-modelling-polish/corpora/pickle/original/lalka-tom-pierwszy.pkl','wb') as g:\n",
        "    pickle.dump(raw_korpus,g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWn1BevVwCG_"
      },
      "outputs": [],
      "source": [
        "with open('.../topic-modelling-polish/corpora/txt/lalka-tom-drugi.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "  raw_korpus = text.split('\\n\\n\\n\\n') #In this corpus, chapters are considered documents\n",
        "  raw_korpus = raw_korpus[2:39]\n",
        "  with open('.../topic-modelling-polish/corpora/pickle/original/lalka-tom-drugi.pkl','wb') as g:\n",
        "    pickle.dump(raw_korpus,g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YEzlrA0MCJu"
      },
      "source": [
        "## Downloading Polish Parliamentary Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Polish Parliamentary Corpus is a huge collection of texts from the plenary sittings of the Sejm and Senate of Polish Republic dating from 1919 to present.\n",
        "\n",
        "In this work, I use only a small sample of texts from different periods that can be downloaded from [this site](http://clip.ipipan.waw.pl/PPC) (around 84,000 tokens).\n",
        "\n",
        "For a longer description of this corpus, visit [its website](https://clarin-pl.eu/index.php/en/kdp-en/).\n",
        "\n",
        "In the rest of the notebook, I refer to this corpus using `parlament_sample` term.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "hZK_gLuhfceM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFTr08zUNG24"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/parlament_corpus/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "K88qWUCEvg-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw2tCEjYNJL6"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('.../topic-modelling-polish/corpora/raw/ppc-sample.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/parlament_corpus/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FQXepMuNMHS"
      },
      "outputs": [],
      "source": [
        "def get_file_paths(directory):\n",
        "    file_paths = []\n",
        "\n",
        "    for root, directories, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            filepath = os.path.join(root, filename)\n",
        "            file_paths.append(filepath)\n",
        "\n",
        "    return file_paths\n",
        "\n",
        "directory = '/content/parlament_corpus/'\n",
        "file_paths = get_file_paths(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6BJ6fflNPyb"
      },
      "outputs": [],
      "source": [
        "def czy_ending(path):\n",
        "  ending = 'text_structure.xml'\n",
        "  if ending in path:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "pliki = list(filter(czy_ending,file_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DetxHeyMNRKp"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "for plik in tqdm.tqdm(pliki):\n",
        "  with open(plik,'r') as f:\n",
        "    file = f.read()\n",
        "\n",
        "    soup = BeautifulSoup(file, 'xml')\n",
        "    text_objects = soup.find_all('u')\n",
        "\n",
        "    texts = []\n",
        "    for txt in text_objects:\n",
        "      texts.append(txt.get_text())\n",
        "\n",
        "    corpus.append(\"\\n\".join(texts))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tsdvCmCBioF"
      },
      "outputs": [],
      "source": [
        "with open('.../topic-modelling-polish/corpora/txt/korpus_parlament_sample.txt','w',encoding='utf-8') as f:\n",
        "  f.writelines(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFmv9IcOL3jH"
      },
      "outputs": [],
      "source": [
        "with open('.../topic-modelling-polish/corpora/pickle/original/korpus_parlament_sample.pkl','wb') as f:\n",
        "  pickle.dump(corpus,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7jRT4B6MMWM"
      },
      "source": [
        "## Corpora preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Corpus preprocessing here is language-specific and uses the language model [pl_core_news_lg](https://spacy.io/models/pl) trained for Polish.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Pjx_EY9uB4K1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4LNHXb9MMkP"
      },
      "outputs": [],
      "source": [
        "def preprocess(corpus):\n",
        "  corpus0 = []\n",
        "  for text in corpus:\n",
        "    text = re.sub(r'\\d+','0',text) #change all the numbers into 0s\n",
        "    text = re.sub(r'\\n+',' ',text)\n",
        "    corpus0.append(text)\n",
        "\n",
        "  return corpus0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-WMY3fOT-FD"
      },
      "outputs": [],
      "source": [
        "def spacy_tokenizer(text):\n",
        "  text_object = nlp(text)\n",
        "  tokenized = [text_raw.text for text_raw in text_object]\n",
        "  return tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9BeaqeQT-Z-"
      },
      "outputs": [],
      "source": [
        "def spacy_lemmatizer(text,nlp):\n",
        "  text_object = nlp(text)\n",
        "  tokenized = [str(text_raw.lemma_) for text_raw in text_object]\n",
        "  return tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lit7NJwDPJm"
      },
      "outputs": [],
      "source": [
        "def spacy_sentenciser(text,nlp):\n",
        "  text_object = nlp(text)\n",
        "  sentences = [str(sentence) for sentence in text_object.sents]\n",
        "\n",
        "  return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCqFJWNqNTGk"
      },
      "source": [
        "## Sentences preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The code below creates a sentence-based version of the corpora. Those particular versions will be utilized specifically for the BERTopic and Top2Vec models. In my experiments, I have found that the version where a document is only one sentence yields the best results in the topic modeling task.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YwRWJpCZgHhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/pl_core_news_lg-3.5.0/pl_core_news_lg-3.5.0.tar.gz"
      ],
      "metadata": {
        "id": "csTTAXopq9nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksf9_TnNieXm"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import itertools\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('pl_core_news_lg')"
      ],
      "metadata": {
        "id": "BeiSEyeixoB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzXY2gvwhiVE"
      },
      "outputs": [],
      "source": [
        "names = ['parlament_sample', 'lit', 'lalka-tom-pierwszy', 'lalka-tom-drugi', 'fakt_ind_nd', 'publ']\n",
        "\n",
        "nlp = spacy.load('pl_core_news_lg')\n",
        "for name in names:\n",
        "  corpus = load_file(f'corpora/pickle/original/{name}')\n",
        "  corpus = preprocess(corpus)\n",
        "  sentencized_corpus = [] #list of sentences from a corpus\n",
        "  for doc in corpus:\n",
        "    sentencized_doc = spacy_sentenciser(doc, nlp=nlp)\n",
        "    if name == 'parlament_sample': #In this corpus, some sentences were very long, which caused problems when using transformer models.\n",
        "    #These were sentences in which the names of MPs were mentioned.\n",
        "      new_sentences = []\n",
        "      for s in sentencized_doc: #Therefore, I divided sentences longer than 80 words into shorter sentences.\n",
        "        if len(s.split()) > 80:\n",
        "          new_sentences.append([' '.join(s.split()[x:x+80]) for x in range(0, len(s.split()), 80)])\n",
        "        else:\n",
        "          new_sentences.append([s])\n",
        "      sentencized_corpus.append(list(itertools.chain(*new_sentences)))\n",
        "    else:\n",
        "      sentencized_corpus.append(sentencized_doc)\n",
        "  sentencized_corpus = [doc for doc in sentencized_corpus if len(doc) > 0]\n",
        "  sentencized_corpus = list(itertools.chain(*sentencized_corpus))\n",
        "  dump_result(sentencized_corpus,f'/corpora/pickle/sentences/{name}_sentences_27lipca')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "Efm8UB9SxOR1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXYjnVnbExUl"
      },
      "source": [
        "## Fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "[FastText](https://fasttext.cc/) is a library and algorithm for efficient text classification and representation learning developed by Facebook's AI Research (FAIR) lab. It utilizes word embeddings to represent words as continuous vectors and trains a shallow neural network for various natural language processing tasks, such as text classification, part-of-speech tagging, and named entity recognition. FastText's key feature is its ability to handle out-of-vocabulary words and its fast training and prediction times, making it popular for tasks involving large text datasets. Here, I will use an already pretrained Fasttext model for Polish in a vector clustering task.\n",
        "\n",
        "I downloaded the model from [the CLARIN-PL website](https://clarin-pl.eu/dspace/handle/11321/606) and then loaded it using the Facebook `fasttext` library, however it is also possible to load the model directly using [HuggingFace](https://huggingface.co/clarin-pl/fastText-kgr10)."
      ],
      "metadata": {
        "id": "Lzv9vcm808dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method presented here is inspired by article\n",
        "\n",
        "Sia, S., Dalmia, A., & Mielke, S. J. (2020). Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too! Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1728–1736. https://doi.org/10.18653/v1/2020.emnlp-main.135."
      ],
      "metadata": {
        "id": "XoOFO5Je4-TF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The presented topic modeling methodology comprises several sequential steps:\n",
        "\n",
        "1. The corpus undergoes lemmatization using the preprocessed Spacy `pl_core_news_lg` model, trained for the Polish language.\n",
        "2. Employing TfidfVectorizer from the `sklearn` library, word weights are computed for individual terms in the corpus. This process involves eliminating words with minimal weights, denoting insignificance in the context of the corpus.\n",
        "3. The pretrained Fasttext model is utilized to obtain word vectors, stored in a dictionary format of word:embedding.\n",
        "4. Subsequently, the dimensionality of the stored vectors is reduced through the application of the UMAP algorithm.\n",
        "5. Employing the KMeans algorithm, vector clustering is performed, with application of the previously derived weights.\n",
        "6. Topics are extracted in the format of topic:list of tuples, each tuple containing (word, weight, vector). This extraction process is based on the top 30 words in each topic, comprising words with the highest weights calculated in step 2.\n",
        "7. Topics are initially output in random order. A subsequent reranking of topics takes place to prioritize those conveying more informative content about the corpus, thus optimizing the readability.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "V2DtvY0q3P9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I import the necessary libraries and define helper functions."
      ],
      "metadata": {
        "id": "OzQqtswO2uEv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-ijOSfUHq69"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext==v0.9.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6-0x2J7Ks6N"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn==0.5.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWRZwuVVKu9Q"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "import pickle\n",
        "import fasttext\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from sklearn.cluster import KMeans\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.manifold import TSNE\n",
        "import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKDEZwHmHTiG"
      },
      "outputs": [],
      "source": [
        "def import_models(model_name):\n",
        "\n",
        "  if model_name == \"FastText100\":\n",
        "    nlp = spacy.load('pl_core_news_lg')\n",
        "    model = fasttext.load_model(\"/content/drive/MyDrive/Projekt magisterski gotowy/models/Fasttext100/kgr10.plain.cbow.dim100.neg10.bin\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxZIr9_GHW1Z"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(vector_model,weighting):\n",
        "  embedding_list = [vector_model.get_word_vector(x) for x in weighting.keys()]\n",
        "  embedding_dict = dict(zip(weighting.keys(),embedding_list))\n",
        "\n",
        "\n",
        "  return embedding_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE7UvGjyEylo"
      },
      "outputs": [],
      "source": [
        "def get_tfidf_scores(corpus):\n",
        "\n",
        "  vectorizer = TfidfVectorizer(use_idf=True)\n",
        "  tfidf_vectorizer_vectors = vectorizer.fit_transform(corpus)\n",
        "  words = vectorizer.get_feature_names_out()\n",
        "  total_tf_idf = tfidf_vectorizer_vectors.toarray().sum(axis=0)\n",
        "\n",
        "\n",
        "  tf_idf_score = {}\n",
        "  for i, word in enumerate(words):\n",
        "    tf_idf_score[word] = total_tf_idf[i]\n",
        "  return tf_idf_score, words.astype(\"str\") #tf_idf_score - dictionary lemma:score, words - list of words that were not rejected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbVihsgNIATi"
      },
      "outputs": [],
      "source": [
        "def cluster(n_clusters,random_state,embeddings, weights=None):\n",
        "\n",
        "  if weights is None:\n",
        "    kmeans = KMeans(n_clusters=n_clusters,random_state=random_state, n_init=100).fit(embeddings)\n",
        "  else:\n",
        "    kmeans = KMeans(n_clusters=n_clusters,random_state=random_state, n_init=100).fit(embeddings,sample_weight = weights)\n",
        "  return kmeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXTdZsRsIEYy"
      },
      "outputs": [],
      "source": [
        "#A function that pulls out words related to a topic\n",
        "def extract_topic_words(clusters,embedding_dict, labels = False, top_n = None, weights = None):\n",
        "\n",
        "  vocabulary = list(embedding_dict.keys())\n",
        "  if labels: #if you pass clusters as a list of numbers\n",
        "    topic_dict = dict(zip(list(vocabulary),clusters))\n",
        "  else: #if you pass clusters as an cluster object\n",
        "    topic_dict = dict(zip(list(vocabulary),clusters.labels_)) #dictionary word:cluster_number\n",
        "\n",
        "  n_topics = len(set(topic_dict.values()))\n",
        "  topic_words_embeddings = [[] for i in range(n_topics)]\n",
        "\n",
        "  for word in topic_dict:\n",
        "    n_topic = topic_dict[word]\n",
        "\n",
        "    if top_n is not None:\n",
        "      word_emb = (word,weights[word],embedding_dict[word])\n",
        "      topic_words_embeddings[n_topic].append(word_emb)\n",
        "    else:\n",
        "      word_emb = (word,embedding_dict[word])\n",
        "      topic_words_embeddings[n_topic].append(word_emb)\n",
        "\n",
        "\n",
        "  if top_n is not None:\n",
        "    final_topic_words_embeddings = []\n",
        "    for topic in topic_words_embeddings:\n",
        "      cut = sorted(topic,key=lambda x: x[1], reverse = True)[:top_n+1]\n",
        "      final_topic_words_embeddings.append(cut)\n",
        "    return final_topic_words_embeddings\n",
        "\n",
        "  return topic_words_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRouuD3KpCBl"
      },
      "source": [
        "In the following cell, a word embedding clustering process is conducted on the corpora utilizing Fasttext, umap, and KMeans. Subsequently, visualizations are generated to provide a representation of the clustered word embeddings as well as the relationship between the Silhouette score and the number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['parlament_sample', 'lit', 'lalka-tom-pierwszy', 'lalka-tom-drugi', 'fakt_ind_nd', 'publ']\n",
        "model = import_models(\"FastText100\")\n",
        "\n",
        "for name in names:\n",
        "  corpus = load_file(f'corpora/pickle/original/{name}')\n",
        "  corpus = preprocess(corpus)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  lemmatized_korpus = [] #lemmatized documents; list of strings\n",
        "  lemmas = [] #lemmatized documents; list of tokens\n",
        "  for doc in tqdm.tqdm(corpus):\n",
        "    lemmatized_doc = spacy_lemmatizer(doc,nlp)\n",
        "    lemmatized_doc = list(map(lambda x:x.lower(),lemmatized_doc))\n",
        "    lemmas.append(lemmatized_doc)\n",
        "    lemmatized_korpus.append(' '.join(lemmatized_doc))\n",
        "  new_lemmatized = []\n",
        "  for l in lemmas:\n",
        "    rob = []\n",
        "    for word in l:\n",
        "      new_word = word.split(' ')\n",
        "      rob.append(new_word)\n",
        "    new_lemmatized.append(list(itertools.chain(*rob)))\n",
        "\n",
        "  weights, vocab = get_tfidf_scores(lemmatized_korpus)\n",
        "  embeddings_dict = get_embeddings(model, weights)\n",
        "  dump_result(embeddings_dict, f'embeddings/fasttext100/{name}_fasttext100_embeddings_dict')\n",
        "  dump_result(weights, f'weights/{name}_tfidf_weights')"
      ],
      "metadata": {
        "id": "2-mRgqmNYqL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd51I0kNINXN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.colors as mcolors\n",
        "names = ['parlament_sample', 'lit', 'lalka-tom-pierwszy', 'lalka-tom-drugi', 'fakt_ind_nd', 'publ']\n",
        "umap_model = umap.UMAP(n_neighbors=15, n_components=5,\n",
        "              min_dist=0.0, metric='cosine', random_state=42)\n",
        "tsne = TSNE()\n",
        "\n",
        "for name in tqdm.tqdm(names):\n",
        "\n",
        "\n",
        "  embeddings_dict = load_file(f'embeddings/fasttext100/{name}_fasttext100_embeddings_dict')\n",
        "  embeddings = list(embeddings_dict.values())\n",
        "  reduced_embeddings = umap_model.fit_transform(embeddings)\n",
        "\n",
        "  scores = load_file(f'weights/{name}_tfidf_weights')\n",
        "  weights = list(scores.values())\n",
        "  #projection = load_file(f'projections/fasttext100/{name}_projection_fasttext100')\n",
        "  projection = TSNE().fit_transform(np.array(reduced_embeddings))\n",
        "  dump_result(projection, f'projections/fasttext100/{name}_projection_fasttext100')\n",
        "  print('Projection done')\n",
        "\n",
        "  sns.set_theme()\n",
        "\n",
        "  silhouette_scores = []\n",
        "  for k in [10, 50, 100, 200]:\n",
        "    kmeans = KMeans(n_clusters=k,random_state=42, n_init=100).fit(reduced_embeddings,sample_weight = weights)\n",
        "    cluster_labels = kmeans.fit_predict(reduced_embeddings)\n",
        "    dump_result(cluster_labels, f'clusters/fasttext100/{name}_{k}_clusters_fasttext100')\n",
        "    silhouette_scores.append(silhouette_score(reduced_embeddings, cluster_labels))\n",
        "    print(f'Clusters for {name}_{k} done')\n",
        "\n",
        "    topics = extract_topic_words(kmeans,embeddings_dict, False, 30, scores)\n",
        "    topic_words = list(map(lambda x: list(x[0]), list(map(lambda x: list(zip(*x)), topics))))\n",
        "    dump_result(topics,f'topics/fasttext100/full topics/{name}_fasttext100_{k}_topics_full_topic')\n",
        "    dump_result(topic_words, f'topics/fasttext100/word lists/{name}_fasttext100_{k}_topics_word_list')\n",
        "\n",
        "    plt.clf()\n",
        "    plot = sns.scatterplot(x=projection[:,0], y=projection[:,1], hue=cluster_labels, palette='Paired', legend=False)\n",
        "    plt.title(f'{name} fasttext, {k} clusters')\n",
        "    plot.figure.savefig(f'.../topic-modelling-polish/figures/fasttext100/clusters/{name}_fasttext100_{k}_clusters.png')\n",
        "    plt.clf()\n",
        "    print(f'Figure title {name} fasttext, {k} clusters done')\n",
        "\n",
        "\n",
        "  df = pd.DataFrame({'clusters':[10,50,100, 200],'score':silhouette_scores})\n",
        "\n",
        "\n",
        "\n",
        "  plot_sil = sns.relplot(\n",
        "    data=df, kind=\"line\",\n",
        "    x=\"clusters\", y=\"score\")\n",
        "  plt.xlabel('Number of clusters')\n",
        "  plt.ylabel('Silhouette score')\n",
        "  plt.title(f'{name}')\n",
        "  plot_sil.savefig(f'.../topic-modelling-polish/figures/fasttext100/silhouette/silhouette_{name}.png')\n",
        "  plt.clf()\n",
        "  print(f'Silhouette figure title {name} done')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJLHsjzg2n8P"
      },
      "source": [
        "### BERTopic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[BERTopic](https://maartengr.github.io/BERTopic/index.html) is a topic modeling technique that leverages Transformers embeddings to cluster and organize documents into coherent topics. It utilizes the contextualized word representations to create document embeddings and then applies a hierarchical clustering algorithm to group similar documents together. Bertopic is particularly effective for unsupervised topic modeling tasks and has shown promising results in various natural language processing applications.\n",
        "\n",
        "\n",
        "BERTopic is highly customizable. I have tested multiple approaches and here I am presenting only the ones I ultimately decided to proceed with.\n",
        "\n",
        "These are BERTopic models utilizing two language models to generate vectors:\n",
        "\n",
        "* [Sentence Transformers](https://www.sbert.net/docs/pretrained_models.html) `paraphrase-multilingual-MiniLM-L12-v2` (BERTopic Multilingual)\n",
        "* Spacy `pl_core_news_lg` (BERTopic Spacy)\n"
      ],
      "metadata": {
        "id": "O1vCrFPUN1xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing clustering results of Sentence Transformers vectors with HDBSCAN and UMAP"
      ],
      "metadata": {
        "id": "Y8fIH0pKNpC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "This section serves as an introduction to testing the BERTopic model's performance. BERTopic can use several pretrained language models, clustering algorithms, and a variety of other features. In this work, HDBSCAN clustering algorithm and Sentence Transformers `paraphrase-multilingual-MiniLM-L12-v2` language model will be applied.\n",
        "\n",
        "Before committing to specific HDBSCAN hyperparameters, I sought to explore their impact on clustering quality of pretrained embeddings generated by this particular model. In this section, I delved into investigating the influence of the `min_cluster_size` hyperparameter. My approach was straightforward, using visualizations to gain insights.\n",
        "\n",
        "In theory, each color on the graphs should represent a distinct cluster. However, in practice, the number of clusters is so vast that colors repeat themselves due to limited color availability. Nevertheless, this method provides valuable assessments of how effectively the algorithm clusters data points.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dgax3KQbNZQs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q-8lyXepmAw"
      },
      "source": [
        "Although in the final version of BERTopic, word vectors are clustered, here I decided to visualize the results of clustering sentence vectors for the sake of implementation simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKcQFNcI2_eO"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers==2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDtDozdh4v5d"
      },
      "outputs": [],
      "source": [
        "!pip install hdbscan==0.8.33\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KozrLLmA4yOi"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn==0.5.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpwEj00V3c-d"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from hdbscan import HDBSCAN\n",
        "import umap\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMFSPhwN-14Y"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "umap_model = umap.UMAP(n_neighbors=15, n_components=5,\n",
        "              min_dist=0.0, metric='cosine', random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuwN1kUr3F_Q"
      },
      "outputs": [],
      "source": [
        "names = ['parlament_sample', 'lit', 'lalka-tom-pierwszy', 'lalka-tom-drugi', 'fakt_ind_nd', 'publ']\n",
        "\n",
        "\n",
        "for name in tqdm.tqdm(names[1:]):\n",
        "  sentences = load_file(f'corpora/pickle/sentences/{name}_sentences')\n",
        "  embeddings = model.encode(sentences)\n",
        "  dump_result(embeddings, f'embeddings/sentence_transformers/{name}_sentence_transformers_embeddings')\n",
        "  embeddings_reduced = umap_model.fit_transform(embeddings)\n",
        "  projection = TSNE().fit_transform(np.array(embeddings_reduced))\n",
        "  dump_result(projection,f'projections/sentence_transformers/{name}_sentence_transformers_projection')\n",
        "  print('Projection done')\n",
        "  for k in [5, 20, 40, 60, 100]:\n",
        "    hdb = HDBSCAN(min_cluster_size = k, cluster_selection_method=\"eom\", prediction_data = True, metric = 'euclidean').fit(embeddings_reduced)\n",
        "    print(f'Clustered for {k} min cluster size')\n",
        "    dump_result(hdb.labels_, f'clusters/sentence_transformers/{name}_{k}_sentence_transformers_labels')\n",
        "\n",
        "    sns.set_theme()\n",
        "\n",
        "\n",
        "    values = np.linspace(0, 1, max(hdb.labels_)+1)\n",
        "    hsv_colormap = mcolors.hsv_to_rgb(np.column_stack([values, np.ones_like(values), np.ones_like(values)]))\n",
        "    custom_palette = sns.color_palette(hsv_colormap)\n",
        "    color_palette = custom_palette\n",
        "    print('Color palette done')\n",
        "    cluster_colors = [color_palette[x] if x >= 0\n",
        "                  else (0.5, 0.5, 0.5)\n",
        "                  for x in hdb.labels_]\n",
        "    print('Cluster colors done')\n",
        "    cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
        "                         zip(cluster_colors, hdb.probabilities_)]\n",
        "    print('Cluster member color done')\n",
        "    plt.clf()\n",
        "    plot = plt.scatter(*projection.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)\n",
        "    print('Plot done')\n",
        "\n",
        "\n",
        "    plt.title(f'{name}, {k} min cluster size')\n",
        "    print('Plot title done')\n",
        "\n",
        "    plot.figure.savefig(f'.../topic-modelling-polish/figures/sentence_transformers/{name}_sentence_transformers_{k}_min_cluster_size.png')\n",
        "    print(f'Figure with the title {name}, {k} min cluster size saved')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCrYNeuGMp89"
      },
      "source": [
        "## BERTopic Multilingual"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the subsequent cells, the finalized approach that has been chosen for implementation is presented.\n",
        "\n",
        "BERTopic allows for a large range of settings. The library's documentation e.g., [Tips&Tricks section](https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html) helped me choose the parameters that are used below.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pf3S1NoAYjaR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF4XGJPTB0h1"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic==v0.14.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVye_JlPIVXU"
      },
      "outputs": [],
      "source": [
        "!pip install keybert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC9IEFBLCbw3"
      },
      "outputs": [],
      "source": [
        "from hdbscan import HDBSCAN\n",
        "import umap\n",
        "import re\n",
        "import itertools\n",
        "import tqdm\n",
        "import pickle\n",
        "from bertopic import BERTopic\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "from keybert import KeyBERT\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic.dimensionality import BaseDimensionalityReduction\n",
        "from bertopic.representation import MaximalMarginalRelevance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acv9YgczqVnR"
      },
      "source": [
        "\n",
        "In the following cell, the BERTopic model is employed for text embedding, utilizing the `paraphrase-multilingual-MiniLM-L12-v2` language model. Subsequently, clustering is performed on these embeddings to group similar semantic representations of the texts. The resulting embeddings for the extracted lemmas from the corpora are then saved for later evalutation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['fakt_ind_nd','publ', 'parlament_sample', 'lit','publ','lalka-tom-pierwszy','lalka-tom-drugi']\n",
        "\n",
        "for name in tqdm.tqdm(names):\n",
        "\n",
        "  file_name = name\n",
        "  corpus = load_file(f'corpora/pickle/sentences/{name}_sentences') #loading sentencized corpora\n",
        "\n",
        "\n",
        "\n",
        "  ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
        "  kw_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')\n",
        "  keywords = kw_model.extract_keywords(corpus)\n",
        "  vocabulary = [k[0] for keyword in keywords for k in keyword]\n",
        "  vocabulary = list(set(vocabulary))\n",
        "  vectorizer_model= CountVectorizer(vocabulary=vocabulary)\n",
        "\n",
        "\n",
        "  umap_model = umap.UMAP(n_neighbors=15, n_components=5,\n",
        "                  min_dist=0.0, metric='cosine', random_state=42)\n",
        "\n",
        "\n",
        "  hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "\n",
        "  representation_model = MaximalMarginalRelevance(diversity=0.3)\n",
        "\n",
        "  topic_model = BERTopic(language='multilingual', ctfidf_model=ctfidf_model, vectorizer_model=vectorizer_model, umap_model=umap_model, hdbscan_model=hdbscan_model,\n",
        "                       representation_model = representation_model,\n",
        "                       verbose=True)\n",
        "  topics, probs = topic_model.fit_transform(corpus)\n",
        "  topic_model.save(f\".../topic-modelling-polish/models/bertopic_multilingual/{file_name}_bertopic_model_multilingual\")\n",
        "\n",
        "\n",
        "\n",
        "  tokenized = []\n",
        "  for doc in corpus:\n",
        "    tokenized_doc = spacy_tokenizer(doc)\n",
        "    tokenized_doc = list(map(lambda x:x.lower(),tokenized_doc))\n",
        "    tokenized.append(tokenized_doc)\n",
        "\n",
        "  words = list(set(list(itertools.chain(*tokenized))))\n",
        "\n",
        "  bertopic_embeddings = topic_model._extract_embeddings(words, method='word', verbose=True)\n",
        "\n",
        "  embedding_dict = {}\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "    embedding_dict[word] = bertopic_embeddings[i]\n",
        "\n",
        "  with open(f'.../topic-modelling-polish/embeddings/bertopic_multilingual/{file_name}_bertopic_multilingual_embeddings_dict.pkl', \"wb\") as f:\n",
        "    pickle.dump(embedding_dict,f)\n"
      ],
      "metadata": {
        "id": "BGpZyizWg5PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpcDRDL_qjGv"
      },
      "source": [
        "In this code cell, the process of extracting topics from the Bertopic model is elucidated. This approach involves the extraction of topics in three distinct versions to enhance the understanding and analysis of the textual data:\n",
        "\n",
        "1. **Raw Form in the Bertopic Model**:\n",
        "   Initially, the raw form of the topics is extracted directly from the Bertopic model. This involves retrieving the topics in their native representation, reflecting the clusters of semantically related words and phrases identified by the model. These raw topics serve as a foundation for further exploration and interpretation.\n",
        "\n",
        "2. **Topics as Word Lists**:\n",
        "   Subsequently, the topics are transformed into curated lists of words. This transformation facilitates better readability and comprehension, as the topics are presented in a structured format, displaying the constituent words within each topic cluster. By organizing the topics in this manner, key themes and concepts that emerge from the text data can be readily identifid and interpreted.\n",
        "\n",
        "3. **Topic Vectors**:\n",
        "   Lastly, the topics can be represented as vectors, encapsulating the essential semantic information within each topic cluster. These vectors serve as compact and meaningful representations of the topics' inherent meaning, facilitating quantitative analysis and enabling sophisticated clustering and similarity comparisons between topics. Those vectors are extracted for later evaluation purposes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQZ3YPD_hG9s"
      },
      "outputs": [],
      "source": [
        "names = ['fakt_ind_nd','publ', 'parlament_sample', 'lit','publ','lalka-tom-pierwszy','lalka-tom-drugi']\n",
        "\n",
        "for name in names:\n",
        "  model_path = f\".../topic-modelling-polish/models/bertopic_multilingual/{name}_bertopic_model_multilingual\"\n",
        "\n",
        "  model = BERTopic.load(model_path)\n",
        "  topics = model.get_topics()\n",
        "  extracted_topics = list(map(lambda x:list(zip(*x))[0],topics.values()))\n",
        "  extracted_topics = list(map(lambda x:list(x),extracted_topics)) #topic_words\n",
        "  topic_vectors = model.topic_embeddings_\n",
        "  dump_result(extracted_topics, f'/topics/bertopic_multilingual/word lists/{name}_bertopic_multilingual_topics_word_list')\n",
        "  dump_result(topics, f'/topics/bertopic_multilingual/full topics/{name}_bertopic_multilingual_full_topics')\n",
        "  dump_result(topic_vectors, f'/embeddings/bertopic_multilingual/topic/{name}_bertopic_multilingual_topic_embeddings')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhzvrLFeN1EM"
      },
      "source": [
        "## BERTopic Spacy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The BERTopic library supports a variety of pre-trained models. The complete list can be found [here](https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html#flair).\n",
        "\n",
        "As a second approach, the same pretrained Spacy model for the Polish language is being tested, which was used for preprocessing the corpora: `pl_core_news_lg`.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "o7l_QpDkfU6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jax>=0.4.9"
      ],
      "metadata": {
        "id": "yhUA7G5D6elu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/pl_core_news_lg-3.5.0/pl_core_news_lg-3.5.0.tar.gz"
      ],
      "metadata": {
        "id": "k4wsXkWjzHIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH63w5u2jyVl"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic[spacy]==v0.14.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBR8PQeKjyfZ"
      },
      "outputs": [],
      "source": [
        "!pip install keybert[spacy]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hdbscan import HDBSCAN\n",
        "import umap\n",
        "import re\n",
        "import itertools\n",
        "import spacy\n",
        "import pickle\n",
        "import tqdm\n",
        "from bertopic import BERTopic\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "from keybert import KeyBERT\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic.dimensionality import BaseDimensionalityReduction\n",
        "from bertopic.representation import MaximalMarginalRelevance"
      ],
      "metadata": {
        "id": "8KQCCcBsmj3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TYkg96_q-gM"
      },
      "source": [
        "In the cell below, the BERTopic model using the language model `pl_core_news_lg` is trained, and subsequently, the vectors for the tokens extracted from the corpora are saved."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['lalka-tom-pierwszy','lalka-tom-drugi','lit','fakt_ind_nd','publ', 'parlament_sample']\n",
        "\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"pl_core_news_lg\", exclude=['tagger', 'parser', 'ner',\n",
        "                                            'attribute_ruler', 'lemmatizer',  'tok2vec', 'morphologizer', '(trainable_lemmatizer)', 'senter'])\n",
        "\n",
        "for name in tqdm.tqdm(names):\n",
        "\n",
        "  corpus = load_file(f'corpora/pickle/sentences/{name}_sentences') #loading sentencized corpora\n",
        "\n",
        "  ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
        "  kw_model = KeyBERT(model=nlp)\n",
        "  keywords = kw_model.extract_keywords(corpus)\n",
        "  vocabulary = [k[0] for keyword in keywords for k in keyword]\n",
        "  vocabulary = list(set(vocabulary))\n",
        "  vectorizer_model= CountVectorizer(vocabulary=vocabulary)\n",
        "\n",
        "\n",
        "  umap_model = umap.UMAP(n_neighbors=15, n_components=5,\n",
        "                  min_dist=0.0, metric='cosine', random_state=42)\n",
        "\n",
        "\n",
        "  hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "\n",
        "  representation_model = MaximalMarginalRelevance(diversity=0.3)\n",
        "\n",
        "  topic_model = BERTopic(embedding_model=nlp, ctfidf_model=ctfidf_model, vectorizer_model=vectorizer_model, umap_model=umap_model, hdbscan_model=hdbscan_model,\n",
        "                       representation_model = representation_model,\n",
        "                       verbose=True)\n",
        "  topics, probs = topic_model.fit_transform(corpus)\n",
        "  topic_model.save(f\".../topic-modelling-polish/models/bertopic_spacy/{name}_bertopic_spacy_model\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  tokenized = []\n",
        "  for doc in corpus:\n",
        "      tokenized_doc = spacy_tokenizer(doc)\n",
        "      tokenized_doc = list(map(lambda x:x.lower(),tokenized_doc))\n",
        "      tokenized.append(tokenized_doc)\n",
        "\n",
        "  words = list(set(list(itertools.chain(*tokenized))))\n",
        "\n",
        "  bertopic_embeddings = topic_model._extract_embeddings(words, method='word', verbose=True)\n",
        "\n",
        "  embedding_dict = {}\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "    embedding_dict[word] = bertopic_embeddings[i]\n",
        "\n",
        "\n",
        "  with open(f'.../topic-modelling-polish/embeddings/bertopic_spacy/{name}_bertopic_spacy', \"wb\") as f:\n",
        "      pickle.dump(embedding_dict,f)\n"
      ],
      "metadata": {
        "id": "pTRQa1owbS7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topics and topic vectors are extracted."
      ],
      "metadata": {
        "id": "nuWUKDgbgGxk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaqrjmsQ-v8J"
      },
      "outputs": [],
      "source": [
        "names = ['fakt_ind_nd','publ', 'parlament_sample', 'lit','publ','lalka-tom-pierwszy','lalka-tom-drugi']\n",
        "\n",
        "for name in names:\n",
        "  model_path = f\".../topic-modelling-polish/models/bertopic_spacy/{name}_bertopic_spacy_model\"\n",
        "  model = BERTopic.load(model_path)\n",
        "  topics = model.get_topics()\n",
        "  extracted_topics = list(map(lambda x:list(zip(*x))[0],topics.values()))\n",
        "  extracted_topics = list(map(lambda x:list(x),extracted_topics)) #topic_words\n",
        "  topic_vectors = model.topic_embeddings_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  dump_result(topics, f'topics/bertopic_spacy/full topics/{name}_bertopic_spacy_full_topics')\n",
        "  dump_result(extracted_topics, f'topics/bertopic_spacy/word lists/{name}_bertopic_spacy_topics_word_list')\n",
        "  dump_result(topic_vectors, f'embeddings/bertopic_spacy/topic/{name}_bertopic_spacy_topic_embeddings')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63JoEwpnPeHt"
      },
      "source": [
        "## Top2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Top2Vec is a topic modeling technique that automatically discovers topics from text data without the need for a pre-defined number of topics. It uses a combination of Word2Vec and Doc2Vec to generate topic vectors, allowing for topic similarity measurements and efficient clustering of documents with similar thematic content. By using embeddings, it can also handle out-of-vocabulary words and scale well with large datasets.\n",
        "\n",
        "You can find more details about the model [here](https://github.com/ddangelov/Top2Vec).\n",
        "\n",
        "There are multiple ways to apply Top2Vec - here, I decided to try out the original algorithm with training the embeddings without the usage of any pretrained models.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4c9pyZiEnrKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jax>=0.4.9"
      ],
      "metadata": {
        "id": "s3UH7p-1BxKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQgQCuxonALL"
      },
      "outputs": [],
      "source": [
        "!pip install top2vec==1.0.29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfvPu9IhnDoc"
      },
      "outputs": [],
      "source": [
        "from top2vec import Top2Vec\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7irwexgsyHE"
      },
      "source": [
        "In the cell below, the model is trained on each corpus and the topics are extracted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HzaG5z9PiOE"
      },
      "outputs": [],
      "source": [
        "names = ['lalka-tom-pierwszy','lalka-tom-drugi','lit','fakt_ind_nd','publ', 'parlament_sample']\n",
        "\n",
        "for name in tqdm.tqdm(names):\n",
        "\n",
        "\n",
        "  corpus = load_file(f'corpora/pickle/sentences/{name}_sentences') #loading sentencized corpora\n",
        "\n",
        "\n",
        "  model = Top2Vec(corpus, speed = 'learn', workers = 8,\n",
        "                hdbscan_args = {'min_cluster_size':5, 'metric':'euclidean', 'cluster_selection_method':'eom', 'prediction_data':True},\n",
        "                umap_args = {'metric':'cosine', 'n_components':5, 'min_dist':0.0, 'random_state':42,'n_neighbors':15},\n",
        "                min_count=5)\n",
        "\n",
        "\n",
        "\n",
        "  model.save(f'.../topic-modelling-polish/models/top2vec/{name}_top2vec_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkpCG7AYmUnS"
      },
      "outputs": [],
      "source": [
        "names = ['fakt_ind_nd','publ', 'parlament_sample', 'lit','publ','lalka-tom-pierwszy','lalka-tom-drugi']\n",
        "\n",
        "for name in names:\n",
        "  model_path = f\".../topic-modelling-polish/models/top2vec/{name}_top2vec_model\"\n",
        "  model = Top2Vec.load(model_path)\n",
        "  full_topics = model.get_topics()\n",
        "  extracted_topics = list(map(lambda x: list(x), full_topics[0]))\n",
        "  topic_embeddings = model.topic_vectors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  dump_result(full_topics, f'topics/top2vec/full topics/{name}_top2vec_full_topics')\n",
        "  dump_result(extracted_topics, f'topics/top2vec/word lists/{name}_top2vec_topics_word_list')\n",
        "  dump_result(topic_embeddings, f'embeddings/top2vec/topic/{name}_top2vec_topic_embeddings')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top2Vec Multilingual"
      ],
      "metadata": {
        "id": "2pCk7lCAFG5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Top2Vec can also be initialized using pre-trained vectors. In this approach, the same model as before will be tested: `paraphrase-multilingual-MiniLM-L12-v2`.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JGiK3GCgdECc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jax>=0.4.9"
      ],
      "metadata": {
        "id": "jR3G8jvII0IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install top2vec[sentence_transformers]"
      ],
      "metadata": {
        "id": "hRUdP54aH6-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from top2vec import Top2Vec\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "4fv0uDb1IFyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['lalka-tom-pierwszy','lalka-tom-drugi','lit','fakt_ind_nd','publ', 'parlament_sample']\n",
        "\n",
        "for name in tqdm.tqdm(names):\n",
        "\n",
        "\n",
        "  corpus = load_file(f'corpora/pickle/sentences/{name}_sentences') #loading sentencized corpora\n",
        "\n",
        "\n",
        "  model = Top2Vec(corpus, embedding_model = 'paraphrase-multilingual-MiniLM-L12-v2', speed = 'learn', workers = 8,\n",
        "                hdbscan_args = {'min_cluster_size':5, 'metric':'euclidean', 'cluster_selection_method':'eom', 'prediction_data':True},\n",
        "                umap_args = {'metric':'cosine', 'n_components':5, 'min_dist':0.0, 'random_state':42,'n_neighbors':15},\n",
        "                min_count=5)\n",
        "\n",
        "\n",
        "\n",
        "  model.save(f'.../topic-modelling-polish/models/top2vec_multilingual/{name}_top2vec_multilingual_model')"
      ],
      "metadata": {
        "id": "QKXYx45yFHGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['fakt_ind_nd','publ', 'parlament_sample', 'lit','publ','lalka-tom-pierwszy','lalka-tom-drugi']\n",
        "\n",
        "for name in names:\n",
        "  model_path = f\".../topic-modelling-polish/models/top2vec/{name}_top2vec_multilingual_model\"\n",
        "  model = Top2Vec.load(model_path)\n",
        "  full_topics = model.get_topics()\n",
        "  extracted_topics = list(map(lambda x: list(x), full_topics[0]))\n",
        "  topic_embeddings = model.topic_vectors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  dump_result(full_topics, f'topics/top2vec_multilingual/full topics/{name}_top2vec_multilingual_full_topics')\n",
        "  dump_result(extracted_topics, f'topics/top2vec_multilingual/word lists/{name}_top2vec_multilingual_topics_word_list')\n",
        "  dump_result(topic_embeddings, f'embeddings/top2vec_multilingual/topic/{name}_top2vec_multilingual_topic_embeddings')"
      ],
      "metadata": {
        "id": "lpY6jtV5KMxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "q4j-na6qgcF1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlLhApHAPryr"
      },
      "source": [
        "## Coherence, unique words, Jaccard diversity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Assessing the quality of topic models is actually a difficult task and it is far from obvious how to do it in an automatic way. Several metrics are often employed to evaluate the effectiveness of topic models, including coherence, unique words, and Jaccard distance measures.\n",
        "\n",
        "**Coherence**: Coherence is a measure that evaluates the interpretability and semantic consistency of topics generated by a model. It quantifies the degree to which words within a topic are related and can be understood as a coherent theme. High coherence values indicate that the words in a topic have strong semantic associations. Common coherence metrics include C_V, C_NPMI, and UMass, which assess word co-occurrence patterns and the strength of their connections.\n",
        "\n",
        "**Unique Words**: The presence of unique words in a topic is another indicator of its quality. A high number of distinct words within a topic suggests that the model has successfully captured diverse aspects of the data. However, an excessively high count of unique words might also indicate noise or lack of topic focus.\n",
        "\n",
        "**Jaccard Distance**: Jaccard distance is a measure of dissimilarity between sets. In the context of topic modeling, it can be used to compare the similarity between the word sets of different topics. By calculating the Jaccard distance between topics, one can assess how distinct or overlapping they are in terms of vocabulary. This measure helps in identifying topics that might be too similar, potentially indicating redundancy in the model's output.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KY7FY3rJaNl6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xQY3kOnRaok"
      },
      "outputs": [],
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import gensim.corpora as corpora\n",
        "import pandas as pd\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbn6f7TiQwy-"
      },
      "outputs": [],
      "source": [
        "#Source: https://github.com/silviatti/topic-model-diversity/blob/master/diversity_metrics.py\n",
        "\n",
        "def proportion_unique_words(topics):\n",
        "    unique_words = set()\n",
        "    for topic in topics:\n",
        "      unique_words = unique_words.union(set(topic))\n",
        "      puw = len(unique_words) / (len(topic) * len(topics))\n",
        "    return puw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Source: https://github.com/silviatti/topic-model-diversity/blob/master/diversity_metrics.py\n",
        "\n",
        "def pairwise_jaccard_diversity(topics):\n",
        "\n",
        "    dist = 0\n",
        "    count = 0\n",
        "    for list1, list2 in combinations(topics, 2):\n",
        "        js = 1 - len(set(list1).intersection(set(list2)))/len(set(list1).union(set(list2)))\n",
        "        dist = dist + js\n",
        "        count = count + 1\n",
        "    return dist/count"
      ],
      "metadata": {
        "id": "Ye-9vre1qc5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(corpus, topic_words):\n",
        "\n",
        "  dictionary = corpora.Dictionary(corpus)\n",
        "\n",
        "  used_corpus = [dictionary.doc2bow(token) for token in corpus]\n",
        "\n",
        "  coherence_model = CoherenceModel(topics=topic_words,\n",
        "                                      texts=corpus,\n",
        "\n",
        "                                      dictionary=dictionary,\n",
        "                                      coherence='c_v', topn=30)\n",
        "  cv = coherence_model.get_coherence()\n",
        "\n",
        "  coherence_model = CoherenceModel(topics=topic_words,\n",
        "                                      texts=corpus,\n",
        "\n",
        "                                      dictionary=dictionary,\n",
        "                                      coherence='c_npmi', topn=30)\n",
        "  c_npmi = coherence_model.get_coherence()\n",
        "\n",
        "  coherence_model = CoherenceModel(topics=topic_words,\n",
        "                                      texts=corpus,\n",
        "                                      corpus=used_corpus,\n",
        "                                      dictionary=dictionary,\n",
        "                                      coherence='u_mass',topn=30)\n",
        "  umass = coherence_model.get_coherence()\n",
        "\n",
        "  uniques = proportion_unique_words(topic_words)\n",
        "\n",
        "  jaccard = pairwise_jaccard_diversity(topic_words)\n",
        "\n",
        "  results = {\"proportion unique words\": uniques, \"cv coherence\": cv, \"c_npmi coherence\": c_npmi, \"u_mass coherence\":umass, \"jaccard\":jaccard}\n",
        "\n",
        "\n",
        "  return results\n",
        "\n"
      ],
      "metadata": {
        "id": "othyoXLgqhy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['fakt_ind_nd','publ', 'parlament_sample', 'lit','publ','lalka-tom-pierwszy','lalka-tom-drugi']\n",
        "models = ['bertopic_multilingual', 'bertopic_spacy', 'top2vec', 'top2vec_multilingual']\n",
        "\n",
        "\n",
        "for name in names:\n",
        "\n",
        "  corpus = load_file(f'corpora/pickle/original/{name}')\n",
        "  tokenized = []\n",
        "  for doc in corpus:\n",
        "    tokenized_doc = spacy_tokenizer(doc)\n",
        "    tokenized_doc = list(map(lambda x:x.lower(),tokenized_doc))\n",
        "    tokenized.append(tokenized_doc)\n",
        "\n",
        "  for model in models:\n",
        "    topics = load_file(f'topics/{model}/word lists/{name}_{model}_topics_word_list')\n",
        "    topics = [topic for topic in topics if \"\" not in topic] #those models used min_cluster_size = 5, so sometimes it happened that Bertopic added empty strings to a topic\n",
        "\n",
        "\n",
        "    results = evaluate(tokenized, topics)\n",
        "    df = pd.DataFrame(results.values(), index=results.keys(), columns=['value'])\n",
        "\n",
        "\n",
        "    df.to_csv(f'.../topic-modelling-polish/results/{model}/metrics/{name}_{model}_topics_results.csv')\n"
      ],
      "metadata": {
        "id": "GFL1_9O4rzvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fasttext100"
      ],
      "metadata": {
        "id": "FxPtck92tfQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['fakt_ind_nd','publ', 'parlament_sample', 'lit','lalka-tom-pierwszy','lalka-tom-drugi']\n",
        "\n",
        "for name in tqdm.tqdm(names):\n",
        "\n",
        "  corpus = load_file(f'corpora/pickle/original/{name}')\n",
        "  corpus = preprocess(corpus)\n",
        "\n",
        "\n",
        "  lemmatized_korpus = [] #lemmatized documents list; list of strings\n",
        "  lemmas = [] #lemmatized documents list; list of lists\n",
        "  for doc in corpus:\n",
        "    lemmatized_doc = spacy_lemmatizer(doc,nlp)\n",
        "    lemmatized_doc = list(map(lambda x:x.lower(),lemmatized_doc))\n",
        "    lemmas.append(lemmatized_doc)\n",
        "    lemmatized_korpus.append(' '.join(lemmatized_doc))\n",
        "  new_lemmatized = []\n",
        "  for l in lemmas:\n",
        "    rob = []\n",
        "    for word in l:\n",
        "      new_word = word.split(' ')\n",
        "      rob.append(new_word)\n",
        "    new_lemmatized.append(list(itertools.chain(*rob)))\n",
        "\n",
        "\n",
        "  for k in [10,100]:\n",
        "\n",
        "    topics = load_file(f'topics/fasttext100/word lists/{name}_fasttext100_{k}_topics_word_list')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    results = evaluate(new_lemmatized,topics)\n",
        "    df = pd.DataFrame(results.values(), index=results.keys(), columns=['value'])\n",
        "\n",
        "\n",
        "    df.to_csv(f'.../topic-modelling-polish/results/fasttext100/metrics/{name}_fasttext100_{k}_topics_results.csv')\n"
      ],
      "metadata": {
        "id": "_CeGQUNf1XL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "247zfINBPwUq"
      },
      "source": [
        "## Cosine similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Another effective method for assessing the quality of topics and their learnt embedding representations involves utilizing cosine similarity as a metric. Cosine similarity is a mathematical measure that evaluates the angular separation between two vectors in a high-dimensional space, providing an indication of their similarity in direction and orientation.\n",
        "\n",
        "Cosine similarity between topics can be used for semantic comparison of corpora.Should two distinct corpora share a substantial number of akin topics, the cosine similarity metric should reflect this coherence, thereby yielding values close to 1 for topic pairs as well as an overall mean value close to 1. However, the efficacy of this approach hinges on the quality of the topic vectors, relying on the ability of individual models to accurately capture the nuances of the corpora and integrate this understanding into the embeddings.\n",
        "\n",
        "In the code snippet below, an implementation for calculating the cosine similarity matrix across all topics in a given pair of corpora can be found. The outcomes and visual representations are accessible within the repository.\n",
        "\n",
        "Both the BERTopic and Top2Vec models offer means to extract topic vectors from their class objects. BERTopic represents these vectors as a weighted average of constituent word vectors within the topic, while Top2Vec incorporates topic representation as a fundamental part of its algorithm.\n",
        "\n",
        "Given that this method compares separately learned vectors from different corpora, it employs both the standard cosine similarity and an adjusted version for a comprehensive evaluation.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6cSAK3aWkN0G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjXjoiWOFc9-"
      },
      "outputs": [],
      "source": [
        "!pip3 install umap-learn==0.5.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp7v8Y60Wll_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from itertools import combinations_with_replacement\n",
        "import umap\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRmwfGT0-Yhf"
      },
      "outputs": [],
      "source": [
        "def adjusted_cosine_similarity(vec1, vec2):\n",
        "    avg_vec1 = np.mean(vec1, axis=1)\n",
        "    avg_vec2 = np.mean(vec2, axis=1)\n",
        "\n",
        "    Cvec1 = vec1 - avg_vec1[:, np.newaxis]\n",
        "    Cvec2 = vec2 - avg_vec2[:, np.newaxis]\n",
        "\n",
        "    n_rows_vec1, n_rows_vec2 = Cvec1.shape[0], Cvec2.shape[0]\n",
        "    sim_matrix = np.zeros((n_rows_vec1, n_rows_vec2))\n",
        "    for u in range(n_rows_vec1):\n",
        "        for v in range(n_rows_vec2):\n",
        "            cos_sim = np.dot(Cvec1[u], Cvec2[v]) / (np.linalg.norm(Cvec1[u]) * np.linalg.norm(Cvec2[v]))\n",
        "            sim_matrix[u, v] = cos_sim\n",
        "\n",
        "    return sim_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf1jU3xwFi--"
      },
      "outputs": [],
      "source": [
        "def compute_topic_similarities(vectors1, vectors2, fig_title, store_path, umap_model, adjust = False):\n",
        "  vectors1 = umap_model.fit_transform(vectors1)\n",
        "  vectors2 = umap_model.fit_transform(vectors2)\n",
        "\n",
        "  if adjust:\n",
        "    similarities = adjusted_cosine_similarity(vectors1,vectors2)\n",
        "\n",
        "  else:\n",
        "    similarities = cosine_similarity(vectors1,vectors2)\n",
        "\n",
        "\n",
        "\n",
        "  mean_similarity = np.mean(similarities)\n",
        "  plt.clf()\n",
        "  fig, ax = plt.subplots(figsize=(6, 6))\n",
        "  plot = sns.heatmap(similarities, ax=ax,annot=False, cmap='coolwarm')\n",
        "  plt.title(f'{fig_title}, mean: {mean_similarity:.2f}')\n",
        "\n",
        "  plot.figure.savefig(store_path)\n",
        "\n",
        "  return similarities, mean_similarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below corresponds to cosine similarity computation for BERTopic and Top2Vec models."
      ],
      "metadata": {
        "id": "7Xs_79UIi-Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['lit','publ', 'parlament_sample', 'fakt_ind_nd','lalka-tom-pierwszy','lalka-tom-drugi']\n",
        "models = ['bertopic_multilingual', 'bertopic_spacy', 'top2vec', 'top2vec_multilingual']\n",
        "adjusted = [False, True]\n",
        "adjusted_encoded = {True:'adjusted', False:\"not_adjusted\"}\n",
        "pairs = list(combinations_with_replacement(names,2))\n",
        "models_encoded = {'bertopic_multilingual':'multilingual', 'bertopic_spacy':'spacy', 'top2vec':'t2v', 'top2vec_multilingual':'t2v_m'}\n",
        "\n",
        "umap_model = umap.UMAP(n_neighbors=15, n_components=5,\n",
        "              min_dist=0.0, metric='cosine', random_state=42)\n",
        "\n",
        "\n",
        "for model in models:\n",
        "  sim = np.zeros(shape=(len(names), len(names)))\n",
        "  df_adjusted = pd.DataFrame(data= sim, index=names,columns=names)\n",
        "  df_not_adjusted = pd.DataFrame(data= sim.copy(), index=names,columns=names)\n",
        "\n",
        "  for pair in tqdm.tqdm(pairs):\n",
        "    embs1 = np.array(load_file(f'embeddings/{model}/topic/{pair[0]}_{model}_topic_embeddings'))\n",
        "    embs2 = np.array(load_file(f'embeddings/{model}/topic/{pair[1]}_{model}_topic_embeddings'))\n",
        "\n",
        "    for a in adjusted:\n",
        "\n",
        "      path_to_store_fig = f'.../topic-modelling-polish/figures/cosine/{model}/corpora_similarity/{adjusted_encoded[a]}/{pair[0]}_{pair[1]}_cosine_similarities_{adjusted_encoded[a]}'\n",
        "      plt.clf()\n",
        "\n",
        "\n",
        "      fig_title = f'{pair[0]} {pair[1]} {models_encoded[model]} cosine {adjusted_encoded[a]}'\n",
        "\n",
        "      similarities, mean_similarity  = compute_topic_similarities(embs1, embs2, fig_title, path_to_store_fig, umap_model, adjust = a)\n",
        "      if a:\n",
        "        df_adjusted.loc[pair[0], pair[1]] = mean_similarity\n",
        "        df_adjusted.loc[pair[1], pair[0]] = mean_similarity\n",
        "      else:\n",
        "        df_not_adjusted.loc[pair[0], pair[1]] = mean_similarity\n",
        "        df_not_adjusted.loc[pair[1], pair[0]] = mean_similarity\n",
        "\n",
        "      #similarities between topics of two corpora\n",
        "      dump_result(similarities, f'results/{model}/cosine/corpora_similarity/{adjusted_encoded[a]}/{pair[0]}_{pair[1]}_{model}_similarities_{adjusted_encoded[a]}')\n",
        "\n",
        "  #mean similarities between pair of corpora\n",
        "  df_adjusted.to_csv(f'.../topic-modelling-polish/results/{model}/cosine/corpora_similarity/adjusted/{model}_mean_similarities_adjusted.csv')\n",
        "\n",
        "  df_not_adjusted.to_csv(f'.../topic-modelling-polish/results/{model}/cosine/corpora_similarity/not_adjusted/{model}_mean_similarities_not_adjusted.csv')\n"
      ],
      "metadata": {
        "id": "u3bVtYNU-mgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity computation for Fasttext is placed in a seperate cell, because the topic vector derivation has to be calculated independently. Topic vector is derived simply as the average of all vectors of a topic."
      ],
      "metadata": {
        "id": "yZXqlw4Cmjma"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10EAraFVm08k"
      },
      "outputs": [],
      "source": [
        "names = ['fakt_ind_nd','publ','lit', 'parlament_sample','lalka-tom-pierwszy','lalka-tom-drugi']\n",
        "pairs = list(combinations_with_replacement(names,2))\n",
        "adjusted = [False, True]\n",
        "adjusted_encoded = {True:'adjusted', False:\"not_adjusted\"}\n",
        "umap_model = umap.UMAP(n_neighbors=15, n_components=5,\n",
        "              min_dist=0.0, metric='cosine', random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_topic_vectors_from_Fasttext(topics):\n",
        "  topic_vectors = []\n",
        "  unzipped = list(map(lambda x: list(zip(*x)),topics))\n",
        "  for tupl in unzipped:\n",
        "    result = np.array(tuple(s * a for s, a in zip(tupl[1], tupl[2])))\n",
        "    topic_vector = np.mean(result,axis=0)\n",
        "    topic_vectors.append(topic_vector)\n",
        "  topic_vectors = np.array(topic_vectors)\n",
        "  return topic_vectors\n",
        "\n",
        "\n",
        "for k in tqdm.tqdm([10,50,100,200]):\n",
        "  sim = np.zeros(shape=(len(names), len(names)))\n",
        "  df_adjusted = pd.DataFrame(data= sim, index=names,columns=names)\n",
        "  df_not_adjusted = pd.DataFrame(data= sim.copy(), index=names,columns=names)\n",
        "  for pair in pairs:\n",
        "\n",
        "    topics1 = load_file(f'topics/fasttext100/full topics/{pair[0]}_fasttext100_{k}_topics_full_topic')\n",
        "    topics2 = load_file(f'topics/fasttext100/full topics/{pair[1]}_fasttext100_{k}_topics_full_topic')\n",
        "\n",
        "    topic_vectors1 = compute_topic_vectors_from_Fasttext(topics1)\n",
        "    topic_vectors2 = compute_topic_vectors_from_Fasttext(topics2)\n",
        "    for a in adjusted:\n",
        "\n",
        "\n",
        "      path_to_store_fig = f'.../topic-modelling-polish/figures/cosine/fasttext100/corpora_similarity/{adjusted_encoded[a]}/{pair[0]}_{pair[1]}_fasttext100_{k}_similarities_{adjusted_encoded[a]}'\n",
        "      plt.clf()\n",
        "\n",
        "      fig_title = f'{pair[0]} {pair[1]} fasttext100 cosine {adjusted_encoded[a]}'\n",
        "\n",
        "      similarities, mean_similarity  = compute_topic_similarities(topic_vectors1, topic_vectors2, fig_title, path_to_store_fig, umap_model, adjust = a)\n",
        "      if a:\n",
        "        df_adjusted.loc[pair[0], pair[1]] = mean_similarity\n",
        "        df_adjusted.loc[pair[1], pair[0]] = mean_similarity\n",
        "      else:\n",
        "        df_not_adjusted.loc[pair[0], pair[1]] = mean_similarity\n",
        "        df_not_adjusted.loc[pair[1], pair[0]] = mean_similarity\n",
        "\n",
        "\n",
        "\n",
        "      dump_result(similarities, f'results/fasttext100/cosine/corpora_similarity/{adjusted_encoded[a]}/{pair[0]}_{pair[1]}_similarities_fasttext100_{k}_{adjusted_encoded[a]}')\n",
        "\n",
        "\n",
        "  df_adjusted.to_csv(f'.../topic-modelling-polish/results/fasttext100/cosine/corpora_similarity/adjusted/fasttext100_{k}_mean_similarities_adjusted.csv')\n",
        "  df_not_adjusted.to_csv(f'.../topic-modelling-polish/results/fasttext100/cosine/corpora_similarity/not_adjusted/fasttext100_{k}_mean_similarities_not_adjusted.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HQ8OhdSP5MB"
      },
      "source": [
        "## Average closest topic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The last measure used in this work for semantic comparison of corpora is the average closest topic. For each topic in a given corpus, the closest topic in the other corpus was found. Then an average was drawn from the found cosine similarity values. This process was repeated for each model and for each pair of corpora.\n",
        "\n",
        "The cell below corresponds again only to BERTopic and Top2Vec models.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UccpEsABonKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['lit','publ', 'parlament_sample', 'fakt_ind_nd','lalka-tom-pierwszy','lalka-tom-drugi']\n",
        "pairs = list(combinations_with_replacement(names,2))\n",
        "models = ['bertopic_multilingual', 'bertopic_spacy', 'top2vec', 'top2vec_multilingual']\n",
        "adjusted = [False, True]\n",
        "adjusted_encoded = {True:'adjusted', False:\"not_adjusted\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for model in tqdm.tqdm(models):\n",
        "  sim = np.zeros(shape=(len(names), len(names)))\n",
        "  df_adjusted = pd.DataFrame(data= sim, index=names,columns=names)\n",
        "  df_not_adjusted = pd.DataFrame(data= sim.copy(), index=names,columns=names)\n",
        "  for pair in pairs:\n",
        "    for a in adjusted:\n",
        "\n",
        "      matrix = load_file(f'results/{model}/cosine/corpora_similarity/{adjusted_encoded[a]}/{pair[0]}_{pair[1]}_{model}_similarities_{adjusted_encoded[a]}')\n",
        "      column_max_values = np.max(matrix,axis=0) #corpus in pair[0]\n",
        "      row_max_values = np.max(matrix,axis=1) #corpus in pair[1]\n",
        "\n",
        "      mean_column = np.mean(column_max_values)\n",
        "      mean_row = np.mean(row_max_values)\n",
        "\n",
        "      if a:\n",
        "        df_adjusted.loc[pair[0], pair[1]] = mean_column\n",
        "        df_adjusted.loc[pair[1], pair[0]] = mean_row\n",
        "      else:\n",
        "        df_not_adjusted.loc[pair[0], pair[1]] = mean_column\n",
        "        df_not_adjusted.loc[pair[1], pair[0]] = mean_row\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    df_adjusted.to_csv(f'.../topic-modelling-polish/results/{model}/cosine/average_closest_topics/adjusted/{model}_average_closest_topic_adjusted.csv')\n",
        "    df_not_adjusted.to_csv(f'.../topic-modelling-polish/results/{model}/cosine/average_closest_topics/not_adjusted/{model}_average_closest_topic_not_adjusted.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ4wZyjyUwVT",
        "outputId": "0ec8e153-dfa4-4e63-e7d8-4dc90520cf0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:17<00:00,  4.27s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irVsjQ2rvIGX"
      },
      "source": [
        "Average clostest topic calculation for Fasttext."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['fakt_ind_nd','publ','lit', 'parlament_sample','lalka-tom-pierwszy','lalka-tom-drugi']\n",
        "pairs = list(combinations_with_replacement(names,2))\n",
        "adjusted = [False, True]\n",
        "adjusted_encoded = {True:'adjusted', False:\"not_adjusted\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for k in tqdm.tqdm([10,50,100,200]):\n",
        "  sim = np.zeros(shape=(len(names), len(names)))\n",
        "  df_adjusted = pd.DataFrame(data= sim, index=names,columns=names)\n",
        "  df_not_adjusted = pd.DataFrame(data= sim.copy(), index=names,columns=names)\n",
        "  for pair in pairs:\n",
        "    for a in adjusted:\n",
        "\n",
        "      matrix = load_file(f'results/fasttext100/cosine/corpora_similarity/{adjusted_encoded[a]}/{pair[0]}_{pair[1]}_similarities_fasttext100_{k}_{adjusted_encoded[a]}')\n",
        "      column_max_values = np.max(matrix,axis=0) #corpus in pair[0]\n",
        "      row_max_values = np.max(matrix,axis=1) #corpus in pair[1]\n",
        "\n",
        "      mean_column = np.mean(column_max_values)\n",
        "      mean_row = np.mean(row_max_values)\n",
        "\n",
        "      if a:\n",
        "        df_adjusted.loc[pair[0], pair[1]] = mean_column\n",
        "        df_adjusted.loc[pair[1], pair[0]] = mean_row\n",
        "      else:\n",
        "        df_not_adjusted.loc[pair[0], pair[1]] = mean_column\n",
        "        df_not_adjusted.loc[pair[1], pair[0]] = mean_row\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  df_adjusted.to_csv(f'.../topic-modelling-polish/results/fasttext100/cosine/average_closest_topics/adjusted/fasttext100_{k}_average_closest_topic_adjusted.csv')\n",
        "  df_not_adjusted.to_csv(f'.../topic-modelling-polish/results/fasttext100/cosine/average_closest_topics/not_adjusted/fasttext100_{k}_average_closest_topic_not_adjusted.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBWiyR19aHjn",
        "outputId": "9fe9475f-0360-4de2-cef8-dffaec6e79ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  4.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reaching the topics"
      ],
      "metadata": {
        "id": "5osjHFHlsEBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "For better readability, the extracted topics are also saved in `json` format. They can be found in the `topics/json/` folder.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LEDzifzrsIqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "C9P7iHoFHEFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeIWL_E4MKOA"
      },
      "outputs": [],
      "source": [
        "def pickle_to_json(topics, model, name, k = None):\n",
        "\n",
        "  if k is not None:\n",
        "    model = f'{model}_{k}'\n",
        "\n",
        "  top_dict = {}\n",
        "  for i,top in enumerate(topics):\n",
        "    top_dict[i] = list(top)\n",
        "\n",
        "  json_object = json.dumps(top_dict, indent=3, ensure_ascii=False).encode('utf-8')\n",
        "\n",
        "  with open(f'.../topic-modelling-polish/topics/json/{model}_{name}_topics.json', 'w', encoding = 'utf-8') as f:\n",
        "      f.write(json_object.decode())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['fakt_ind_nd','publ', 'lit','parlament_sample','lalka-tom-pierwszy','lalka-tom-drugi']\n",
        "models = ['bertopic_multilingual', 'bertopic_spacy', 'top2vec', 'fasttext100', 'fasttext100', 'top2vec_multilingual']\n",
        "\n",
        "\n",
        "for name in names:\n",
        "  for model in models:\n",
        "    if model == 'fasttext100':\n",
        "      for k in [10,100]:\n",
        "        topics = load_file(f'topics/fasttext100/word lists/{name}_fasttext100_{k}_topics_word_list')\n",
        "        pickle_to_json(topics, model, name, k = k)\n",
        "    else:\n",
        "      topics = load_file(f'topics/{model}/word lists/{name}_{model}_topics_word_list')\n",
        "      pickle_to_json(topics, model, name)\n",
        "\n"
      ],
      "metadata": {
        "id": "aAg9CHXFs2rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reranking Fasttext topics"
      ],
      "metadata": {
        "id": "5fzitqZ6rev1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Topics found with the Fasttext model appear in the list in random order. While many of these topics are of a very good quality - they are consistent and seem to effectively mirror the content of the corpora — certain topics appear to have been formulated based on orthographic resemblances rather than semantic relevance. The introduction of a reranking approach holds the potential to enhance the clarity of outcomes.\n",
        "\n",
        "In the subsequent section, a simple reranking technique is applied, showcasing observable beneficial outcomes in result filtration. This approach capitalizes on three key variables: the mean edit distance between all words within a given topic (accounting for orthographic similarities), the mean cosine similarity inherent to the topic (encompassing semantic coherence), and the average TF-IDF weight associated with the topic. The latter is computed by deriving the average of the weights assigned to each word within the topic, offering insights into the significance of these words within the overall structure of the corpus.\n",
        "\n",
        "In the following section, reranking will be applied to the topics extracted from the parliamentary corpus (in the version of 100 topics).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pCezSBC6rjW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import editdistance"
      ],
      "metadata": {
        "id": "e9Mvrnpvhiv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_cosine_similarity(vectors):\n",
        "    vectors = [np.array(vector) for vector in vectors]\n",
        "    num_vectors = len(vectors)\n",
        "    total_similarity = 0\n",
        "\n",
        "    for i in range(num_vectors):\n",
        "        for j in range(i + 1, num_vectors):  # Only calculate upper triangular part (excluding diagonal)\n",
        "            similarity = np.dot(vectors[i], vectors[j]) / (np.linalg.norm(vectors[i]) * np.linalg.norm(vectors[j]))\n",
        "            total_similarity += similarity\n",
        "\n",
        "    num_pairs = num_vectors * (num_vectors - 1) / 2  # Number of unique pairs (excluding self-similarity and reverse pairs)\n",
        "    avg_similarity = total_similarity / num_pairs\n",
        "\n",
        "    return avg_similarity"
      ],
      "metadata": {
        "id": "OvGDr3AlpBgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_edit_distanc(str_list):\n",
        "    total_distance = 0\n",
        "    num_pairs = 0\n",
        "\n",
        "    for i in range(len(str_list)):\n",
        "        for j in range(i + 1, len(str_list)):\n",
        "            distance = editdistance.eval(str_list[i], str_list[j])\n",
        "            total_distance += distance\n",
        "            num_pairs += 1\n",
        "\n",
        "    if num_pairs > 0:\n",
        "        mean_distance = total_distance / num_pairs\n",
        "    else:\n",
        "        mean_distance = 0\n",
        "\n",
        "    return mean_distance"
      ],
      "metadata": {
        "id": "dgzAl2HorfgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = load_file('weights/parlament_sample_tfidf_weights')\n",
        "embeddings = load_file('embeddings/fasttext100/parlament_sample_fasttext100_embeddings_dict')\n",
        "topics = load_file('topics/fasttext100/word lists/parlament_sample_fasttext100_100_topics_word_list')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "top_dict = {}\n",
        "top_weight = {}\n",
        "epsilon = 1e-9\n",
        "for i,top in enumerate(topics):\n",
        "  l = []\n",
        "  mean_weight = 0\n",
        "  vectors = []\n",
        "  for j,word in enumerate(list(top)):\n",
        "    mean_weight += weights[word]\n",
        "    l.append(str(word))\n",
        "    vectors.append(embeddings[word])\n",
        "\n",
        "  av_sim = average_cosine_similarity(vectors)\n",
        "  mean_edit_dist = mean_edit_distanc(l)\n",
        "  top_weight[i] = (mean_weight/ len(top), av_sim, mean_edit_dist)\n",
        "  top_dict[i] = l\n",
        "\n",
        "srt_edit = sorted(top_weight.values(), key = lambda x: x[2])\n",
        "srt_weight = sorted(top_weight.values(), key = lambda x: x[0])\n",
        "srt_av_sim = sorted(top_weight.values(), key = lambda x: x[1])\n",
        "min_edit = srt_edit[0][2]\n",
        "max_edit = srt_edit[-1][2]\n",
        "min_weight = srt_weight[0][0]\n",
        "max_weight = srt_weight[-1][0]\n",
        "min_av_sim = srt_av_sim[0][1]\n",
        "max_av_sim = srt_av_sim[-1][1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "top_weight_2 = {}\n",
        "for key, value in top_weight.items():\n",
        "  try:\n",
        "\n",
        "    score = ((value[0] - min_weight) / (max_weight - min_weight)) * 0.5 +  math.log(((value[1] - min_av_sim) / (max_av_sim - min_av_sim)) * 0.3 + epsilon) + math.log(((value[2] - min_edit) / (max_edit - min_edit)) *0.4 + epsilon)\n",
        "    top_weight_2[key] = score\n",
        "  except ValueError:\n",
        "    print((key,value))\n",
        "sorted_dict_with_lists = {\n",
        "    key: top_dict[key] for key, _ in sorted(top_weight_2.items(), key=lambda x: x[1], reverse=True)\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open('.../topic-modelling-polish/topics/json/reranked/parlament_sample_topics_fasttext100_100_reranked.json', 'w', encoding ='utf8') as json_file:\n",
        "    json.dump(sorted_dict_with_lists, json_file, ensure_ascii = False, indent = 4)\n"
      ],
      "metadata": {
        "id": "zmz8OGMvM0z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further steps to enhancing the results, e.g. stopwords removal, could also be considered.\n",
        "\n",
        "The outcomes of the aforementioned procedure can be located within the `topics/json/reranked` directory. The application of the reranking technique led to the elevation of topics associated with parliament, politics, the nation, etc. This outcome aligns precisely with the thematic content anticipated for the parliamentary corpus!"
      ],
      "metadata": {
        "id": "9DrS3VnLgcLk"
      }
    }
  ]
}